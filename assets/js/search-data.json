{
  
    
        "post0": {
            "title": "Kaggle Titanic with Linear Regression and Neural Network",
            "content": "About . This blog post and the efforts behind this are heavily inspired by Prof. Jeremy Howard and the FastAI community! . In lesson 3 of the v2022 course, Prof. Howard takes us on a ride where he explains Machine Learning in a very simple and intuitive way. The theme of the lesson (unlike others) is no-code. We understand linear regression and Neural Nets from scratch by simple operations in Microsoft Excel. . To understand thoroughly, I tried my hand at replicating what Jeremy did in the lecture – in excel. As I tried to implement it, I realized that Python is way more intuitive for me and I will learn more if I replicate this in Python. I am not much familiar with Excel and doing it in Python seemed like a good learning exercise. . I had to figure out a few things along the way. Getting the dimensions of the matrices right and handling NumPy arrays was a pain but worth the try. Ultimately, I believe I have implemented the Lesson 3 excel file of the Kaggle Titanic dataset here in Python. . A quick note about structure: . This notebook is a walkthrough and written with the intent to have every functionality that Jeremy has introduced in Lesson 3 Titanic walkthrough. | I did not want to take the help of any off-the-shelf Machine Learning frameworks so we build everything from scratch. Only Pandas and Numpy libraries are used here. | Even for the solver, we build Gradient Decent from scratch with NumPy and import it here. | Feel free to look at solver.py in the repository (in ./supporting_files/2022-11-01-FastAI-Lesson3-Titanic-Python directory). It has linear regression and deep learning versions of Gradient Decent algorithms. | Any comments/suggestions are super welcome! | . | . Note: We implement this with Matrix multiplication logic which Jeremy explained in the latter part of the lecture. . Step1: Let&#39;s improt libraries &#8211;&#160;pandas and numpy and have our solvers ready. . import pandas as pd import numpy as np from solver import gradient_decent, gradient_decent_nn . TRAIN_FILE_PATH = os.path.join(supporting_files_dir, &quot;titanic&quot;, &quot;train.csv&quot;) ITERATIONS = 80 . def load_data(file_path): &quot;&quot;&quot; loads csv and returns the dataframe &quot;&quot;&quot; df = pd.read_csv(file_path) return df . Step 2: Let&#39;s load the data, and observe the columns . df = load_data(file_path=TRAIN_FILE_PATH) df.columns . Index([&#39;PassengerId&#39;, &#39;Survived&#39;, &#39;Pclass&#39;, &#39;Name&#39;, &#39;Sex&#39;, &#39;Age&#39;, &#39;SibSp&#39;, &#39;Parch&#39;, &#39;Ticket&#39;, &#39;Fare&#39;, &#39;Cabin&#39;, &#39;Embarked&#39;], dtype=&#39;object&#39;) . Step 3: We&#39;ll follow lecture 3 as close as possible. . Here, we&#39;ll scoop only the columns that Jeremy has used in the lecture. We will also enrich our data-frame with the same logic as the lecture. We will prepare new columns for class, embarked, fare, and male/female. . Note: The overall flow and even the column names are kept the same as in the excel file. We replace Ones column with Const in dataframe so that it alights with Const feature in the features list. . nec_cols = [&quot;Survived&quot;, &quot;Embarked&quot;, &quot;Pclass&quot;, &quot;Sex&quot;, &quot;Age&quot;, &quot;Fare&quot;, &quot;SibSp&quot;, &quot;Parch&quot;] df_nec = df[nec_cols] df_nec.columns . Index([&#39;Survived&#39;, &#39;Embarked&#39;, &#39;Pclass&#39;, &#39;Sex&#39;, &#39;Age&#39;, &#39;Fare&#39;, &#39;SibSp&#39;, &#39;Parch&#39;], dtype=&#39;object&#39;) . Same as the lecture, since many rows are empty, we&#39;ll only get non-empty rows. We get around 712 rows of data. . df_nec = df_nec.dropna(how=&#39;any&#39;,axis=0) . Now we need to enrich our dataframe i.e. make new features based on the data. . Per the lecture, we follow new columns for features: Pclass, Embark, Age, and Fare. Let&#39;s apply similar logic and transformation . max_age = df_nec[&quot;Age&quot;].max() print(f&quot;Max age is: {max_age}&quot;) df_nec[&quot;Age_N&quot;] = df_nec[&quot;Age&quot;] / max_age df_nec[&quot;log_Fare&quot;] = np.log10(df_nec[&quot;Fare&quot;]+1) # Pclass_1 &amp; Pclass_2 df_nec[&quot;Pclass_1&quot;] = (df_nec[&quot;Pclass&quot;] == 1).astype(int) df_nec[&quot;Pclass_2&quot;] = (df_nec[&quot;Pclass&quot;] == 2).astype(int) # Embark_S and Embark_C df_nec[&quot;Embark_S&quot;] = (df_nec[&quot;Embarked&quot;] == &quot;S&quot;).astype(int) df_nec[&quot;Embark_C&quot;] = (df_nec[&quot;Embarked&quot;] == &quot;C&quot;).astype(int) # Male df_nec[&quot;Male&quot;] = (df_nec[&quot;Sex&quot;] == &quot;male&quot;).astype(int) df_nec[&quot;Const&quot;] = 1 Y = df_nec[&quot;Survived&quot;].values.reshape(len(df_nec),1) # df_nec.columns . Max age is: 80.0 . Step 4: Initialize the weights . Now that we have our data ready, we initialize the weights randomly. Columns R-AA in excel sheet. . params_features = [&quot;SibSp&quot;, &quot;Parch&quot;, &quot;Age_N&quot;, &quot;log_Fare&quot;, &quot;Pclass_1&quot;, &quot;Pclass_2&quot;, &quot;Embark_S&quot;, &quot;Embark_C&quot;, &quot;Male&quot;, &quot;Const&quot;] df_nec_num = df_nec[params_features] np.random.seed(0) rand_initializers = np.random.uniform(low=0, high=1, size=(len(params_features), 1)).round(2) . rand_initializers . array([[0.55], [0.72], [0.6 ], [0.54], [0.42], [0.65], [0.44], [0.89], [0.96], [0.38]]) . Step 5: Cost and Prediction walkthrough (Linear Regression) . Cost and Prediction functions are defined in the following cells for understanding. These are used for calculating cost and predictions of our model. . For cost, we take the sum as opposed to average in the excel file. . def compute_cost(predictions, Y): &quot;&quot;&quot; calculate cost based on two column vectors of size n x 1 &quot;&quot;&quot; m = len(Y) cost_per_datapoint = np.subtract(predictions, Y)**2 total_cost = sum(cost_per_datapoint) # total_cost = sum(cost_per_datapoint/(2*m)) return total_cost def compute_model_output(X, W): # model: X*W # W = n x 1, # X = n x m # n: number of features (weights) + constant term # m: number of training examples # output: predictions (n x 1) return np.matmul(X, W) . Step 6: Use above functions and optimize the weights. (Linear Regression) . We optimize the weights from random numbers with an ojective to minimize loss/cost defined above. . weights, costs = gradient_decent(iterations=ITERATIONS, W=rand_initializers, X=df_nec_num, Y=Y, alpha=0.1) . Iteration # 0, cost: [6936.00736642] Iteration # 8, cost: [298.12898267] Iteration # 16, cost: [250.16788143] Iteration # 24, cost: [216.89097022] Iteration # 32, cost: [192.44287041] Iteration # 40, cost: [174.63487796] Iteration # 48, cost: [160.97752133] Iteration # 56, cost: [150.915957] Iteration # 64, cost: [143.19206508] Iteration # 72, cost: [137.17995749] . Step 7: Neural Network implementation . Note that we only implement the Neural Network as shown in the lecture and this is purely for understanding purposes. There is a possibility that production Neural Nets work in a slightly polished/refined way. The underlying components remain the same. . For neural nets, we do the following computes: . We calculate linear regression twice with a different set of randomly initialized feature weights. | Then we apply ReLU – which is where we replace negative values in our predictions with zero. | Finally, we add these two individual predictions up and call Neural Network predictions. | . Following is the function we use for calculating the predictions of a Nural Network. Logic follows the excel file and lecture 3. . def compute_model_output_nn(X, W): # model: X*W # W = n x 1, # X = n x m # n: number of features (weights) + constant term # m: number of training examples # output: predictions (n x 1) linear_preds = np.matmul(X, W) # this returns the matrix of dimentions (n x 2). Each column as separate linear regression prediction. # now, we apply ReLU to the predictions lin_relu_preds = np.where(linear_preds &lt; 0, 0, linear_preds) # make negative values zero # now, we take sum of these two predictions return np.sum(lin_relu_preds, axis=1) # sum . np.random.seed(0) rand_initializers_dl = np.random.uniform(low=0, high=1, size=(len(params_features), 2)).round(2) . weights_dl, costs_dl = gradient_decent_nn(iterations=ITERATIONS, W=rand_initializers_dl, X=df_nec_num, Y=Y, alpha=0.1) . Iteration # 0, cost: [37825.41313583] Iteration # 8, cost: [228.30451932] Iteration # 16, cost: [184.92278215] Iteration # 24, cost: [159.92333415] Iteration # 32, cost: [145.00889701] Iteration # 40, cost: [135.83319126] Iteration # 48, cost: [130.18663049] Iteration # 56, cost: [126.39428926] Iteration # 64, cost: [123.72175152] Iteration # 72, cost: [121.67674539] . Step 8: [Optional] Try to intuitively understand efficiency of learning. . Here we try to make sense of Linear Regression vs. Neural Network algorithms costs after training them with the same number of pre-defined iterations. . We plot the Loss vs. Training Iterations graph of both Linear Regression and Neural Network algorithms. It&#39;s evident that Neural Network learns more effectively and has consistently lower loss as opposed to the Linear Regression algorithm. . This highlights the beauty of Neural Nets where they represent the data more effectively. . Note: The Y axis of the graph is in the log scale. As Jeremy mentioned: &quot;The Log scale makes really big numbers small and does not change small numbers much so it&#39;s easier to understand in certain cases.&quot; . import matplotlib.pyplot as plt from matplotlib.pyplot import figure figure(figsize=(12, 8), dpi=80) plt.plot(costs, lw=3) plt.plot(costs_dl, lw=3) plt.yscale(&#39;log&#39;) plt.legend([&#39;Linear Regression&#39;, &#39;Neural Network with ReLU&#39;], loc=&#39;upper right&#39;) plt.title(&#39;Loss wrt. Iterations: Linear Regression vs. Neural Network&#39;) plt.xlabel(&#39;Iterations&#39;, fontsize=14) plt.ylabel(&#39;Loss&#39;, fontsize=14) plt.show() .",
            "url": "https://thepegasos.github.io/blog/2022/11/01/FastAI-Lesson3-Titanic-Python.html",
            "relUrl": "/2022/11/01/FastAI-Lesson3-Titanic-Python.html",
            "date": " • Nov 1, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Linear Regression from Scratch",
            "content": "About . This notebook is a quick walkthough of how Linear Regression really works. We will not be using any external Machine Learning library (except for numpy, which really is a scientific computing libray than a Machine Learning one). . Every piece of the algorithm (right from model, to cost function, until gradinent decent) will be built from scratch. . What We Need . From the outset, as explained in the blog post, we need three important things to build any Machine Learning model . Data from which We Need to Learn/represent | Model | Cost Function | Optimization Algorithm (we&#39;d be using Gradient Decent here as most production tasks use some or the other variation of this algorithm and as it suits our purpse here) | Part 1: The Data . Your dataset can contain anywhere between a few hundred to few million different datapoints. These datapoints are fed into an Optimization Algorithm multiple times. With each iteration, the Gradient Decent (Optimzation Algorithm), calculates the Output of the model, computes the cost based on that computation and real value (comes from the data itself), and then updates the parameters to minimize the loss. . For our case here, we will be just using 2 data points. x = [10, 20], with corresponding y = [105, 205]. . import numpy as np x_train = np.array([10, 20]) y_train = np.array([105, 205]) # also some test data x_test = np.array([30, 40]) y_test = np.array([305, 405]) . Part 2: The Model . Let&#39;s now build the function which will represent our model. Since we are doing linear regression, we will be using: y = wx + b . Here w is a weight and b is a bias term – they&#39;re generally referred as model parameters. . &gt; Important: All in all, all we need is the right parameters that will represent the data well. i.e. Machine Learning is really the process of obtaining the best parameters w and b that fits our data well – also called as training. Once we have these parameters, we use it to predict unseen data, also called as inference. These two steps are all there is to it. We&#39;ll be doing both in this tutorial! . Now, lets build a function that will help us compute the output y, based on model wx + b . def compute_model_output(w, x, b): # print(f&quot;w: {w}, x: {x}, b: {b}&quot;) model_output = w*x + b return model_output . Let&#39;s assume that our input x is 2. Our learned parameters are w=10, b=5. Thus our output should be 10 2 + 5*, which is 25. Let&#39;s verify: . compute_model_output(w=10, x=2, b=5) . 25 . As you can guess, as we have y = [105, 205]. when we&#39;re done, we&#39;d like to have w=10, and b=5. Let&#39;s see how we can arrive at these values! . That&#39;s correct! This means our codified model in Python is returning the expected the values. Feel free to try here with your own examples here. . Part 3: The Cost Function . Now, as explained the blog post, we need a way to measure the performance of our learned model. . i.e. we need a way to measure if our model is producing realistic values. . def compute_cost(y_pred, y, m): # y_pred = shape: (1, m), contains the predicted values by the Model # y = shape: (1, m), contains the ground truth (in our case, 105, 205) cost_per_datapoint = (y_pred - y)**2 # this variable contains the cost per datapoint total_cost = sum(cost_per_datapoint)/(2*m) return total_cost . Part 4: The Optimization Algorithm . def gradient_decent(iterations, w, x, b, y, alpha): &quot;&quot;&quot; returns the learned parameters w, b and list of cost per iteration &quot;&quot;&quot; m = len(x) all_costs = [] for i in range(iterations): model_prediction = compute_model_output(w, x, b) cost = compute_cost(y_pred=model_prediction, y=y, m=m) dw,db = compute_gradients(model_prediction, y, x, m) w, b = update_parameters(w, b, dw, db, alpha) all_costs.append(cost) print_at = iterations/10 if i%print_at == 0: # only print 10 progress costs print(f&quot;Iteration # {i}, cost: {cost}&quot;) return w, b, all_costs . def compute_gradients(y_pred, y, x, m): diff_per_example = (y_pred - y)/m dj_dw = diff_per_example*x dj_db = diff_per_example return dj_dw, dj_db . def update_parameters(w, b, dw, db, alpha): w = w - alpha*sum(dw) b = b - alpha*sum(db) return w, b . Now that we have all the componenets, let&#39;s train our model using our data! . w_init = 10 b_init = 3 iterations = 1000 alpha = 0.0001 w, b, all_costs = gradient_decent(iterations, w_init, x_train, b_init, y_train, alpha) . Iteration # 0, cost: 2.0 Iteration # 100, cost: 0.2093535825341508 Iteration # 200, cost: 0.19784645051025793 Iteration # 300, cost: 0.19738372726634273 Iteration # 400, cost: 0.19699033884724906 Iteration # 500, cost: 0.19659815826785115 Iteration # 600, cost: 0.19620676109658386 Iteration # 700, cost: 0.19581614315416213 Iteration # 800, cost: 0.19542630287300639 Iteration # 900, cost: 0.19503723870480114 . print(f&quot;Learned parameters: w: {w}, b: {b}, Expected parameters: w: {10}, b: {5}&quot;) . Learned parameters: w: 10.118430918661392, b: 3.026938061811292, Expected parameters: w: 10, b: 5 . And.. congratulations!, you have now built the linear regression machine learning model from scratch!! .",
            "url": "https://thepegasos.github.io/blog/jupyter/2022/07/23/Linear-Regression-from-Scratch.html",
            "relUrl": "/jupyter/2022/07/23/Linear-Regression-from-Scratch.html",
            "date": " • Jul 23, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "What is Machine Learning?",
            "content": "What is Machine Learning? . . Machine Learning is the process of teaching a computer from your data. The outcome of task and data at hand decides what and how we want to teach. There are different algorithms of Machine Learning, viz. Linear Regression, Decision Trees, Neural Networks, etc. Before we get into individual algorithms, it’s important to know and understand at a high level how Machine Learning Algorithms work. . What does it mean by implementing a Machine Learning algorithm? What are it’s core components, if any? What broad steps are there? We will try to answer these here. . A Machine Learning algorithm revolves around “something” to be learnt from given data. “Something” could be a classification task – classify dog vs cat image from a pool of cats and dogs images or it could be a problem of predicting a continuous value (e.g. house price, stock price, etc.) – this is also called as Regression. Don’t worry if you never heard word regression before (I did not know what it really meant either, even after looking into a dictionary, I could not relate to it intuitively. As you might have guessed, English is not my first language :) ) Having spent some time on finding it’s true meaning, just trust me when I say, don’t worry about it, you don’t need to know it. . Now, coming to the task at hand. We want to teach our algorithm a task. Think over it, if you want to teach someone something, how would you do it? What tools, measures/metrics would you fomulate? Think about your approach for a bit before moving to next part. . From the outset, you will need: . A way to predict – to compute what your algorithm “thinks” about the input. This will be our instrument to get the response when we provide with raw data. Standard term for this is Model. This can also be thought as a hypothesis/assumption that the our process has learnt. | Then , we need a way to measure it’s (model’s) performance – how well your model has learnt the task that you are trying to teach. We call this a Cost Function | Once you have prediction and a way to measure it’s performance, you need a way to optimize your model’s performance based on it’s Cost. We call this an Optimization Algorithm. This is the most crucial part, running optimization algorithm with Model predictions and Cost Function calculations is so called Training. | Analogy . Broadly, you can draw an anology of teaching something to your younger self or a toddler, like: . If we want to teach them something, we would not just read it out load once and assume that they understood everything. Interestingly, same goes with machines/computers. . We need to tell it once (train for one epoch), | Ask some questions, see how well it understood by asking them to take quizzes (measure performance (calculate cost) on trained data) | And continue this process until it understands per our standards (repeat training until convergence). | . Note that in above anology, in the first step, we did not have to come up with so called *model* for the kid. The brain of the kid itslef is a model in this scenario. Once their performance is satisfactory during teaching along with quiz scores, we give then an unseen questions in the form of an exam (run our algorithm against test data). The score of that exam decides how well the toddler learnt and if it can generalize well. Same goes for our model, based on the TestScore, we decide if this meets the standars for deploying it in prodcution. . . Core Components . Concretely, we can draw 3 important things that we need in our toolset to teach machine a given task: 1 . A Model | A Cost Function | An Optimization Algorithm to minimize the Cost | Metrics to decide if the algorithm performs well on unseen data (e.g. accuracy, loss, etc.) | . Sometimes metric and Cost Function can be same. BUT Metrics are mostly understandable by humans while Cost Functions are for optimizing algorithms Let’s look at how you would go on implementing each of these steps in Python code in the next post! . Om Shanti, Indra. .",
            "url": "https://thepegasos.github.io/blog/markdown/2022/07/16/What-is-Machine-Learning.html",
            "relUrl": "/markdown/2022/07/16/What-is-Machine-Learning.html",
            "date": " • Jul 16, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. asd asdf will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://thepegasos.github.io/blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://thepegasos.github.io/blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Industry expert with 8 years of experience, looking forward to the decades to come. . | Expertise in Python, Ansible, and Red Hat Linux. . | Python Frameworks: Django, PyTorch, Pandas, Requests, TensorFlow. . | .",
          "url": "https://thepegasos.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://thepegasos.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}