{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression from Scratch\n",
    "> A tutorial of building and training a linear regression model \n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [jupyter]\n",
    "- image: images/chart-preview.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About\n",
    "\n",
    "This notebook is a quick walkthough of how Linear Regression really works. We will not be using any external Machine Learning library (except for `numpy`, which really is a scientific computing libray than a Machine Learning one). \n",
    "\n",
    "Every piece of the algorithm (right from model, to cost function, until gradinent decent) will be built from scratch. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What We Need\n",
    "From the outset, as explained in the [blog post](https://thepegasos.github.io/blog/2022/07/16/What-is-Machine-Learning.html), we need three important things to build any Machine Learning model \n",
    "1. Data from which We Need to Learn/represent\n",
    "2. Model\n",
    "2. Cost Function \n",
    "3. Optimization Algorithm (we'd be using <b>Gradient Decent</b> here as most production tasks use some or the other variation of this algorithm <b>and</b> as it suits our purpse here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: The Data \n",
    "Your dataset can contain anywhere between a few hundred to few million different datapoints. These datapoints are fed into an Optimization Algorithm multiple times. With each iteration, the Gradient Decent (Optimzation Algorithm), calculates the Output of the model, computes the cost based on that computation and real value (comes from the data itself), and then updates the parameters to minimize the loss. \n",
    "\n",
    "For our case here, we will be just using 2 data points. ***x = [10, 20]***, with corresponding ***y = [105, 205]***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x_train = np.array([10, 20])\n",
    "y_train = np.array([105, 205])\n",
    "\n",
    "# also some test data\n",
    "x_test = np.array([30, 40])\n",
    "y_test = np.array([305, 405])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now build the function which will represent our model. Since we are doing linear regression, we will be using: ***y = wx + b***\n",
    "\n",
    "Here ***w*** is a weight and ***b*** is a bias term – they're generally referred as *model parameters*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`> Important: All in all, all we need is the right parameters that will represent the data well. \n",
    "i.e. Machine Learning is really the process of obtaining the best parameters w and b that fits our data well – also called as training. \n",
    "Once we have these parameters, we use it to predict unseen data, also called as inference. These two steps are all there is to it. We'll be doing both in this tutorial!`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets build a function that will help us compute the output ***y***, based on model ***wx + b***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_model_output(w, x, b):\n",
    "    # print(f\"w: {w}, x: {x}, b: {b}\")\n",
    "    model_output = w*x + b\n",
    "    return model_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume that our input ***x*** is 2. Our learned parameters are ***w=10, b=5***. Thus our output should be ***10\\*2 + 5***, which is ***25***. Let's verify: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_model_output(w=10, x=2, b=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can guess, as we have ***y = [105, 205]***. when we're done, we'd like to have ***w=10, and b=5***. Let's see how we can arrive at these values!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's correct! This means our codified model in Python is returning the expected the values. Feel free to try here with your own examples here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: The Cost Function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, as explained the [blog post](https://thepegasos.github.io/blog/2022/07/16/What-is-Machine-Learning.html), we need a way to measure the performance of our learned model. \n",
    "\n",
    "i.e. we need a way to measure if our model is producing realistic values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(y_pred, y, m):\n",
    "    # y_pred = shape:  (1, m), contains the predicted values by the Model\n",
    "    # y = shape: (1, m), contains the ground truth (in our case, 105, 205)\n",
    "\n",
    "    cost_per_datapoint = (y_pred - y)**2 # this variable contains the cost per datapoint\n",
    "    total_cost = sum(cost_per_datapoint)/(2*m)\n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: The Optimization Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_decent(iterations, w, x, b, y, alpha):\n",
    "    \"\"\"\n",
    "    returns the learned parameters w, b and list of cost per iteration\n",
    "    \"\"\"\n",
    "    m = len(x)\n",
    "    all_costs = []\n",
    "    for i in range(iterations):\n",
    "        \n",
    "        model_prediction = compute_model_output(w, x, b)\n",
    "        cost = compute_cost(y_pred=model_prediction, y=y, m=m)\n",
    "        dw,db = compute_gradients(model_prediction, y, x, m)\n",
    "        w, b = update_parameters(w, b, dw, db, alpha)\n",
    "        all_costs.append(cost)\n",
    "        print_at = iterations/10\n",
    "        if i%print_at == 0: # only print 10 progress costs\n",
    "            print(f\"Iteration # {i}, cost: {cost}\")\n",
    "    return w, b, all_costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradients(y_pred, y, x, m):\n",
    "    diff_per_example = (y_pred - y)/m\n",
    "    dj_dw = diff_per_example*x\n",
    "    dj_db = diff_per_example\n",
    "    return dj_dw, dj_db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(w, b, dw, db, alpha):\n",
    "    w = w - alpha*sum(dw)\n",
    "    b = b - alpha*sum(db)\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all the componenets, let's train our model using our data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration # 0, cost: 2.0\n",
      "Iteration # 100, cost: 0.2093535825341508\n",
      "Iteration # 200, cost: 0.19784645051025793\n",
      "Iteration # 300, cost: 0.19738372726634273\n",
      "Iteration # 400, cost: 0.19699033884724906\n",
      "Iteration # 500, cost: 0.19659815826785115\n",
      "Iteration # 600, cost: 0.19620676109658386\n",
      "Iteration # 700, cost: 0.19581614315416213\n",
      "Iteration # 800, cost: 0.19542630287300639\n",
      "Iteration # 900, cost: 0.19503723870480114\n"
     ]
    }
   ],
   "source": [
    "w_init = 10\n",
    "b_init = 3\n",
    "iterations = 1000\n",
    "alpha = 0.0001\n",
    "w, b, all_costs = gradient_decent(iterations, w_init, x_train, b_init, y_train, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned parameters: w: 10.118430918661392, b: 3.026938061811292, Expected parameters: w: 10, b: 5\n"
     ]
    }
   ],
   "source": [
    "print(f\"Learned parameters: w: {w}, b: {b}, Expected parameters: w: {10}, b: {5}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And.. **congratulations!**, you have now built the linear regression machine learning model from scratch!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.13 ('base369')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "9115efdd0dae74a370c547359129cb687b38dbefadbd5a364331c5a20e2fe3fc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
